{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy consumption prediction models for efficient, environment-friendly metered building energy usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "To build a model to predict energy consumption of buildings without retrofits aimed at assessing the performance of existing retrofits for efficient, environment-friendly metered building energy usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation:\n",
    "\n",
    "This proposition is not my own but was devised by the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE), and I was captivated by this idea after coming across the Kaggle's competition website.\n",
    "\n",
    "The competition is currently live and ends December 19, 2019. This suits the curriculum of the Data Incubator program, which in turn keeps my motivation higher and stay focused on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "Metered building energy usage helps us consume fuel and water wisely while inspiring us to improve building efficiencies to reduce costs and emissions by installing and/or improving retrofit components of the buildings.\n",
    "\n",
    "Under the pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model, which forms the objective of this project. \n",
    "\n",
    "Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type, or don't work with different building types. Building counterfactual models to estimate the usage seems the only way to assess the energy consumption.\n",
    "\n",
    "Through this project, I intend to work toward building accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters.\n",
    "\n",
    "The dataset is for public use but available to Kaggle users (upon registration), and is hosted by ASHRAE at kaggle.com website. The data comes from hourly meter readings taken during a three-year timeframe of over 1000 buildings located at different sites worldwide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files:\n",
    "> #### train.csv\n",
    "- contains building_id, meter, timestamp, meter_reading\n",
    "- building_id is the unique id referenced in other data files to enable merging of data\n",
    "- the meter has 4 codes, namely, {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.\n",
    "- timestamp is the time the measurement taken\n",
    "- meter_reading is the target variable, i.e. the energy consumption in kWh (or equivalent units).\n",
    "\n",
    "> #### building_meta.csv\n",
    "- contains site_id, building_id, primary_use, square_feet, year_built, floor_count\n",
    "- site_id is the unique id for the area where the building is located, and enables merging of weather data\n",
    "- building_id is the unique id of the building, enables merging of train and test data\n",
    "- primary_use is the indicator of the primary category of activities of the building based on the EnergyStar property type definitions\n",
    "- square_feet is the Gross floor area of the building\n",
    "- year_built is the year building was opened\n",
    "- floor_count is the number of floors of the building\n",
    "\n",
    "> #### weather_train/test.csv\n",
    "- contains site_id, air_temperature, cloud_coverage, dew_temperature, precip_depth_1_hr, sea_level_pressure, wind_direction, wind_speed\n",
    "- site_id connects with the building_meta data file\n",
    "- air_temperature and dew_temperature in deg celcius, precip_depth_1_hr\n",
    "- precip_depth_1_hr in millimeters\n",
    "- sea_level_pressure in Millibar/hectopascals\n",
    "- cloud_coverage is the portion of the sky covered in clouds\n",
    "- wind_direction is the compass direction i.e. in 0 to 360 degrees\n",
    "- wind_speed in meters/second\n",
    "\n",
    "> #### test.csv\n",
    "The submission files use row numbers for ID codes in order to save space on the file uploads. test.csv has no feature data; it exists so you can get your predictions into the correct order. Here the row_id refers to the row id for your submission file\n",
    "- building_id - Building id code\n",
    "- meter - The meter id code\n",
    "- timestamp - Timestamps for the test data period\n",
    "\n",
    "> #### sample_submission.csv\n",
    "A valid sample submission.\n",
    "\n",
    "All floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload.\n",
    "There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored. \n",
    "\n",
    "\n",
    "### Evaluation Metric:\n",
    "\n",
    "The ASHRAE evaluates the model using the metric `Root Mean Squared Logarithmic Error (RMSLE)`.\n",
    "\n",
    "The RMSLE is calculated as:\n",
    "\n",
    "$ ϵ=1n∑i= \\sqrt{ 1/n (log(pi+1)−log(ai+1))^2 } $\n",
    "Where:\n",
    "\n",
    "- ϵ is the RMSLE value (score)\n",
    "- n is the total number of observations in the (public/private) data set,\n",
    "- pi is your prediction of target, and\n",
    "- ai is the actual target for i.\n",
    "- log(x) is the natural logarithm of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing / Exploratory Data Analysis:\n",
    "\n",
    "Since the data is already cleaned and in a suitable format for processing in Python platform, we don't need to preprocess the data. However, since the data storage is not optimized for memory space, I looked at options of reducing the data size by assigining the feature columns appropriate data types with due care to not to cause any loss / effects on further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc, math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "import tqdm\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as us_cal\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11,8)})\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metadata_ = pd.read_csv('building_metadata.csv')\n",
    "train_df = pd.read_csv('train.csv', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv('test.csv', parse_dates=['timestamp'])\n",
    "weather_train_df = pd.read_csv('weather_train.csv', parse_dates=['timestamp'])\n",
    "weather_test_df = pd.read_csv('weather_test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.shape, weather_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df['site_id'].unique(), weather_test_df['site_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.concat([weather_train_df,weather_test_df],ignore_index=True)\n",
    "\n",
    "temp_skeleton = weather[['site_id', 'timestamp', 'air_temperature']].drop_duplicates(subset=['site_id', 'timestamp']).sort_values(by=['site_id', 'timestamp']).copy()\n",
    "\n",
    "# calculate ranks of hourly temperatures within date/site_id chunks\n",
    "temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "\n",
    "# create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "\n",
    "# Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "site_ids_offsets.index.name = 'site_id'\n",
    "\n",
    "def timestamp_align(df):\n",
    "    df['offset'] = df.site_id.map(site_ids_offsets)\n",
    "    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n",
    "    df['timestamp'] = df['timestamp_aligned']\n",
    "    del df['timestamp_aligned']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df = timestamp_align(weather_train_df)\n",
    "weather_test_df = timestamp_align(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather, df_2d, temp_skeleton, site_ids_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "weather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
